# History of Neural Networks

Week: 8ì£¼ì°¨

![img1](Image3_1/img1.png)

# AlexNet

ê°„ë‹¨í•œ CNN architecture

<aside>
ğŸ’¡ Overall architecture: Conv - Pool - Conv - Pool - FC â€“ FC

- Convolution: 5x5 filters with stride 1
- Pooling: 2x2 max pooling with stride 2
</aside>

![img2](Image3_1/img2.png)

Alexnetì€ ë‘ ê°ˆë˜ì˜ pathê°€ ì¡´ì¬í•œë‹¤. ë‹¹ì‹œë§Œ í•´ë„ ì´ networkë¥¼ ê°ë‹¹í•  memoryê°€ ë¶€ì¡±í•˜ì˜€ê¸°ì— í•œ ë²ˆì— ì§„í–‰í•˜ì§€ ì•Šê³  ë‘ ê°ˆë˜ë¡œ ë‚˜ëˆ„ì–´ 2ê°œì˜ GPUë¥¼ ì‚¬ìš©í–ˆë‹¤ê³  í•œë‹¤.

path1(ìœ—ë¶€ë¶„) : ì£¼ë¡œ colorì™€ ìƒê´€ì—†ëŠ” feature ì¶”ì¶œ

path2(ì•„ë«ë¶€ë¶„) : ì£¼ë¡œ colorì™€ ê´€ë ¨ëœ featureì„ ì¶”ì¶œ

Alexnetì˜ ì£¼ìš” íŠ¹ì§•ìœ¼ë¡œëŠ”

1. 11x11 convolution filter 
    
    â†’ ì´ì „ ë„¤íŠ¸ì›Œí¬ì— ë¹„í•´ í•„í„° ì‚¬ì´ì¦ˆê°€ ëŠ˜ì–´ë‚¨ì— ë”°ë¼ input imageì˜ ì‚¬ì´ì¦ˆë„ í•¨ê»˜ ì¦ê°€í–ˆë‹¤.
    
2. ReLU ì‚¬ìš©
    
    â†’ sigmoidë‚˜ tanhë¥¼ ì‚¬ìš©í–ˆì„ ë•Œë³´ë‹¤ 6ë°° ì´ìƒ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.
    

# VGGNet

![img3](Image3_1/img3.png)

- Deeper architecture : 16 and 19 layers
- Simple architecture : No local response normalization, only 3x3 conv filters blocks, 2x2 max pooling
- Better Performance : Alexnetì— ë¹„í•´ í˜„ì €íˆ ìš°ìˆ˜í•œ ì„±ëŠ¥
- Better generalization : ë‹¤ë¥¸ taskì—ì„œë„ ì„±ëŠ¥ì´ ì˜ ë‚˜ì˜¤ëŠ” ìš°ìˆ˜í•œ ì¼ë°˜í™” ì„±ëŠ¥ ë³´ìœ 
- í° conv filter ëŒ€ì‹  3x3 conv layerë¥¼ ì—¬ëŸ¬ê°œ ìŒ“ìŒìœ¼ë¡œì¨ ì ì€ íŒŒë¼ë¯¸í„°ë¡œë„ í° receptive field í˜•ì„± ê°€ëŠ¥
    - receptive field : ì¶”ì¶œëœ ì–´ë– í•œ featureì— ëŒ€í•´ ì…ë ¥ì˜ìƒì´ ë¯¸ì¹˜ëŠ” ì˜ì—­

# GoogLeNet

**Key Idea : Inception module**

ë‹¤ì–‘í•œ í•„í„°ë¥¼ ì ìš©í•˜ì—¬ concatenate í•œë‹¤.

![img4](Image3_1/img4.png)

ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì„ ë³´ë©´ **1x1 convolution**ì„ ì‚¬ìš©í•˜ëŠ”ë° ì´ê²ƒì´ GoogLeNetì˜ í•µì‹¬ì´ë‹¤.

![img5](Image3_1/img5.png)

ì´ë ‡ê²Œ 1x1 convolutionì„ ì—¬ëŸ¬ê²¹ ìŒ“ìœ¼ë©´ filterì˜ ê°¯ìˆ˜ë§Œí¼ ì±„ë„ì´ ìƒê¸°ì§€ë§Œ ê³µê°„ì ì¸ í¬ê¸°ëŠ” ë°”ë€Œì§€ ì•ŠëŠ”ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.

<aside>
â“ ì±„ë„ì´ ëŠ˜ì–´ë‚˜ëŠ” ê²ƒì˜ ê¸ì •ì ì¸ ì˜ë¯¸ëŠ”?

</aside>

<aside>
ğŸ’¡ Overall architecture

- Stem network : vanilla convolution networks
- Stacked inception modules
- Auxiliary classifiers : back propagation ì¤‘ vanishing gradient ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì¤‘ê°„ì¤‘ê°„ gradientë¥¼ ì…ë ¥í•´ì£¼ëŠ” ê²ƒ â†’ ë§¨ ë°‘ê¹Œì§€ gradientë¥¼ ë³´ë‚´ì¤„ ìˆ˜ ìˆê²Œë¨ (í•™ìŠµë•Œë§Œ ì‚¬ìš©)
- Classifier output (a single FC layer)
</aside>

# ResNet

ê·¼ë˜ì— ìì£¼ ì‚¬ìš©ë˜ëŠ” model

![[img6] CIFAR-10 ë°ì´í„°ì— ëŒ€í•œ training error(ì™¼ìª½) ê·¸ë¦¬ê³  test error(ì˜¤ë¥¸ìª½)](Image3_1/img6.png)

**Degradation** ë¬¸ì œ ë°œìƒ : ì–´ëŠì •ë„ ì´ìƒ ê¹Šì–´ì§„ ë„¤íŠ¸ì›Œí¬ì—ì„œ vanishing/exploding gradient ë¬¸ì œ ë•Œë¬¸ì— ì„±ëŠ¥ì´ ë” ë–¨ì–´ì§€ëŠ” ë¬¸ì œ

**â†’ ResNetì€ ì´ë¥¼ í•´ê²° í•˜ê¸° ìœ„í•´ shortcut connectionì„ ì‚¬ìš©**

![[img7] ê¸°ì¡´ neural netê³¼ residual net](Image3_1/img7.png)


Plain net ë³´ë‹¤ Residual netì´ ë” í•™ìŠµì´ ì‰½ë‹¤ëŠ” ê°€ì •ì„ ì‚¬ìš©í–ˆë‹¤.

<aside>
ğŸ’¡ Overall architecture

![img8](Image3_1/img8.png)

- Convolutional Layer : 3x3 Filter ì‚¬ìš© (íŠ¹ì§•ë§µì˜ í¬ê¸°ê°€ ê°™ì€ ë ˆì´ì–´ëŠ” í•„í„°ì˜ ê°¯ìˆ˜ë„ ê°™ê³ , íŠ¹ì§•ë§µì˜ í¬ê¸°ê°€ ì ˆë°˜ìœ¼ë¡œ ì¤„ì–´ë“¤ë©´ í•„í„°ì˜ ê°¯ìˆ˜ëŠ” ë‘ë°°ë¡œ ì¦ê°€ì‹œí‚´)
- Pooling Layer : Stride = 2ì˜ Downsampling ì ìš©, Fully-Connected Layer ì´ì „ì— Avg Pooling ì ìš©
- Residual Network : ë§¤ ë‘ ì¥ì˜ ë ˆì´ì–´ë§ˆë‹¤ Shortcut ì‚½ì… (xì™€ H(x)ì˜ Dimensionì„ ë§ì¶”ê¸° ìœ„í•´ Zero-Paddingê³¼ 1x1 Conv Layer ì ìš©)
- ì´ ë ˆì´ì–´ ê°¯ìˆ˜ : 34
</aside>

# Beyond ResNet

1. DenseNet
2. SENet
3. EfficientNet

### EfficientNet

Deep, Wide, and high Resolution networks

![img9](Image3_1/img9.png)

(b),(c),(d) ëª¨ë‘ ì„±ëŠ¥ í–¥ìƒì— ë„ì›€ì´ ë˜ëŠ” ë°©ë²•ì¸ë° ì´ë“¤ì„ ì ì ˆíˆ ì„ì–´ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë‚´ëŠ” ëª¨ë¸ì´ë‹¤.

ì˜¤ë¥¸ìª½ ê·¸ë˜í”„ì—ì„œ ê²€ì •ìƒ‰ ì„ ì€ ì‚¬ëŒì´ ì†ìœ¼ë¡œ ë§Œë“  ëª¨ë¸ì¸ë° ì´ê²ƒë³´ë‹¤ ë¹¨ê°„ ì„ ì¸ EfficientNetì´ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤!

# Summary..

**AlexNet**

- simple CNN architecture
- Simple computation, heavy memory size
- Low accuracy

**VGGNet**

- simple with 3x3 convolutions
- highest memory, the heaviest computation

**GoogLeNet**

- inception module and auxiliary classifier

**ResNet** 

- deeper layers with residual blocks
- moderate efficiency

<aside>
â˜‘ï¸ **GoogLeNetì´ ê°€ì¥ íš¨ìœ¨ì ì¸ CNN modelì´ë‹¤. í•˜ì§€ë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë³µì¡í•˜ë‹¤.
ê·¸ë ‡ê¸°ì— ìš”ì¦˜ì€ ë‹¨ìˆœí•˜ì§€ë§Œ ê°•ë ¥í•œ VGGNet ì´ë‚˜ ResNetì„ backbone modelë¡œ ì‚¬ìš©í•œë‹¤.**

</aside>